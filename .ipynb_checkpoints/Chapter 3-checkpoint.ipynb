{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f12c7edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a88e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c538b529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53a3f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env):\n",
    "\n",
    "    #set the number of iterations\n",
    "    num_iterations = 1000\n",
    "\n",
    "    #set the threshold number for checking the convergence of the value function\n",
    "    threshold = 1e-20\n",
    "\n",
    "    #we also set the discount factor\n",
    "    gamma = 1.0\n",
    "\n",
    "    #now, we will initialize the value table, with the value of all states to zero\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "\n",
    "    #for every iteration\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        #update the value table, that is, we learned that on every iteration, we use the updated value\n",
    "        #table (state values) from the previous iteration\n",
    "        updated_value_table = np.copy(value_table)\n",
    "\n",
    "        #now, we compute the value function (state value) by taking the maximum of Q value.\n",
    "\n",
    "        #thus, for each state, we compute the Q values of all the actions in the state and then\n",
    "        #we update the value of the state as the one which has maximum Q value as shown below:\n",
    "        for s in range(env.observation_space.n):\n",
    "\n",
    "            Q_values = [sum([prob*(r + gamma * updated_value_table[s_])\n",
    "                             for prob, s_, r, _ in env.P[s][a]])\n",
    "                                   for a in range(env.action_space.n)]\n",
    "\n",
    "            value_table[s] = max(Q_values)\n",
    "\n",
    "        #after computing the value table, that is, value of all the states, we check whether the\n",
    "        #difference between value table obtained in the current iteration and previous iteration is\n",
    "        #less than or equal to a threshold value if it is less then we break the loop and return the\n",
    "        #value table as our optimal value function as shown below:\n",
    "\n",
    "        if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n",
    "             break\n",
    "\n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6767225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(value_table):\n",
    "    \n",
    "    # set the discount factor\n",
    "    gamma = 1.0\n",
    "    \n",
    "    # first we initialize the policy with zeros, that is, first we set the action for all the states of \n",
    "    # be zero\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    #now, compute the Q function using the optimal value function obtained from the \n",
    "    #previous step. After computing the Q function, we can extract policy by selecting action which has\n",
    "    #maximum Q value. Since, we are computing the Q function using the optimal value \n",
    "    #function, the policy extracted from the Q function will be the optimal policy.\n",
    "    \n",
    "    #As shown below, for each state, we compute the:Q values for all the actions in the state and\n",
    "    #then we extract policy by selecting the action which has maximum Q value.\n",
    "    \n",
    "    #for each state:\n",
    "    for s in range(env.observation_space.n):\n",
    "        \n",
    "        # compute the Q value of the all actions in the staet:\n",
    "        Q_values = [sum([prob*(r + gamma * value_table[s_])\n",
    "                       for prob, s_, r, _ in env.P[s][a]])\n",
    "                           for a in range(env.action_space.n)]\n",
    "        \n",
    "        # extract policy by selecting the action which has maximum Q value\n",
    "        policy[s] = np.argmax(np.array(Q_values))\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd3149a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_value_function = value_iteration(env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8ddce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy = extract_policy(optimal_value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ace5e49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 3. 3. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f66296",
   "metadata": {},
   "source": [
    "# Solving Frozen Lake with Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab339845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7fa60f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "379c7cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value_function(policy):\n",
    "    \n",
    "    # now, let's define the number of iteration\n",
    "    num_iterations = 1000\n",
    "    \n",
    "    # define the threshold value\n",
    "    threshold = 1e-20\n",
    "    \n",
    "    # set the discount factor \n",
    "    gamma = 1.0\n",
    "    \n",
    "    #now, we will initialize the value table, with the all states to zero\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    # for every iteration\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # update the value table, that is, we learned that on every iteration, we use the updated value\n",
    "        # table (state value) from previous iteration\n",
    "        updated_value_table = np.copy(value_table)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # thus, for each state, we select the action according to the given policy and then we update the\n",
    "        # value of the state using the selected action as shown below:\n",
    "        \n",
    "        # for each state\n",
    "        for s in range(env.observation_space.n):\n",
    "            \n",
    "            # select the action in the state according to the policy\n",
    "            a = policy[s]\n",
    "            \n",
    "            # compute the value of the state using the selected action\n",
    "            value_table[s] = sum([prob * (r + gamma * updated_value_table[s_])\n",
    "                                 for prob, s_, r, _ in env.P[s][a]])\n",
    "            \n",
    "            # after computing the value table, that is, value of all the states, we check whether the \n",
    "            # difference between value table obtained in the current iteration and previous iteration is\n",
    "            # Less than or equal to a threshold value if it is less then we break the loop and return the\n",
    "            # value table as an accurate value function of the given policy\n",
    "            \n",
    "            if (np.sum((np.fabs(updated_value_table - value_table))) <= threshold ):\n",
    "                break\n",
    "                \n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b72dd7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_table = np.zeros(env.observation_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ec034c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d80dadd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51441e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 0, 0.0, False),\n",
       " (0.3333333333333333, 4, 0.0, False),\n",
       " (0.3333333333333333, 1, 0.0, False)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "822ed604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(value_table):\n",
    "    \n",
    "    # set the discount factor \n",
    "    gamma = 1.0\n",
    "    \n",
    "    # first, we initialize the policy with zeros, that is, first, we set the actions for all the states to\n",
    "    # be zero\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    # now, we compute the Q function using the optimal value function obtained from the\n",
    "    # previous step. After computing the Q function, we can extract policy by selecting action which has\n",
    "    # maximum Q value. Since we are computing the Q function using the optimal value\n",
    "    # function, the policy extracted from the Q function will be the optimal policy.\n",
    "    \n",
    "    \n",
    "    # As shown below, for each state, we compute the Q values for all the actions in the state and\n",
    "    #  the we extract the policy by selecting the action which has maximum Q value.\n",
    "    \n",
    "    # for each state \n",
    "    for s in range(env.observation_space.n):\n",
    "        \n",
    "        # compute the Q value of all the actions in the state\n",
    "        Q_values = [sum([prob * (r + gamma * value_table[s_])\n",
    "                        for prob, s_, r, _ in env.P[s][a]])\n",
    "                           for a in range(env.action_space.n)]\n",
    "        \n",
    "        # extract policy by selecting the action which has maximum Q value\n",
    "        policy[s] = np.argmax(np.array(Q_values))\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d155cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env):\n",
    "    \n",
    "    # set the number of iterations\n",
    "    num_iterations = 1000\n",
    "    \n",
    "    # We learned that in the policy iteration method, we begin by initializing a random policy.\n",
    "    # So we will initliaze the random policy which selects the action 0 in all the states \n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    # for every iteration\n",
    "    for i in range(num_iterations):\n",
    "        # Compute the value function using the policy\n",
    "        value_function = compute_value_function(policy)\n",
    "        \n",
    "        # extract the new policy from the computed value function\n",
    "        new_policy = extract_policy(value_function)\n",
    "        \n",
    "        # if the policy and new_policy are same then break the loop\n",
    "        if (np.all(policy==new_policy)):\n",
    "            break\n",
    "            \n",
    "        # else, update the current policy to new_policy\n",
    "        policy = new_policy\n",
    "        \n",
    "    return policy\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14ed8753",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy = policy_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e416e890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b695a8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
