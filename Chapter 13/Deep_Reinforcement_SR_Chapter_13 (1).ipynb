{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing PPO Cliped Method"
      ],
      "metadata": {
        "id": "58G4WoGBckbs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR4sQvAJcKhU",
        "outputId": "5f2f1749-af87-4cdc-928b-ab8874767eef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.14.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym"
      ],
      "metadata": {
        "id": "vX8mSKSXdGXH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a clear understanding of how the PPO works, we use Tensorflow in the non-eager mode by disabling Tensorflow 2 behavior"
      ],
      "metadata": {
        "id": "LPZmaLD_dpLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEFduqkKdWLY",
        "outputId": "d03e1f2f-e584-48a0-f5a8-9650643a9291"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the Gym Environment\n",
        "Let's create pendulum environment using gym:"
      ],
      "metadata": {
        "id": "mGgxiYkYgtg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Pendulum-v1').unwrapped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X91MGMAgq1E",
        "outputId": "fd362ec0-ef15-4395-a621-47dccbeae064"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the state shape of the environment"
      ],
      "metadata": {
        "id": "SV0FzPqQj0ZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_shape = env.observation_space.shape[0]\n",
        "print(state_shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apupbY8IhJfB",
        "outputId": "035a7771-e1c0-4c5b-ad21-322dd694b20f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the action shape of the environment"
      ],
      "metadata": {
        "id": "6JV7XUcokHFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action_shape = env.action_space.shape[0]\n",
        "\n",
        "print(action_shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuD8mjqBkY9d",
        "outputId": "f0a9cf3f-bab1-48c6-9d4d-da72d808388e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the pendulum is a continuous environment and thus our action space consists of continuous values. So, we get the bound of our action space:"
      ],
      "metadata": {
        "id": "_4E7IYTZknAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action_bound = [env.action_space.low, env.action_space.high]"
      ],
      "metadata": {
        "id": "RqSwpgfEkfY8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(action_bound)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXlwxelmlMbK",
        "outputId": "e6ec263c-407d-4ebf-fa30-cd523bb6bbdf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([-2.], dtype=float32), array([2.], dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set epsilon value which is used in the clipped objective"
      ],
      "metadata": {
        "id": "ETMVW8qNlUrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.2"
      ],
      "metadata": {
        "id": "bmBtRZOrlPTq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LAggqn02m3xT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the PPO (Proximal Policy Optimization) Class"
      ],
      "metadata": {
        "id": "xSD-p22Xm4Xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PPO(object):\n",
        "  def __init__(self):\n",
        "\n",
        "    # Start the tensorflow tension\n",
        "    self.sess = tf.Session()\n",
        "\n",
        "    # define the placeholder for the state\n",
        "    self.state_ph = tf.placeholder(tf.float32, [None, state_shape], 'state')\n",
        "\n",
        "    # now, let's build the value network which returns the value of a state\n",
        "    with tf.variable_scope('value'):\n",
        "      layer1 = tf.layers.dense(self.state_ph, 100, tf.nn.relu)\n",
        "      self.v = tf.layers.dense(layer1, 1)\n",
        "\n",
        "      # define the placeholder for Q value\n",
        "      self.Q = tf.placeholder(tf.float32, [None, 1], 'discounted_r')\n",
        "\n",
        "      # define the advantage value as the difference between the Q value and state value\n",
        "      self.advantage = self.Q - self.v\n",
        "\n",
        "      # compute the loss of the network\n",
        "      self.value_loss = tf.reduce_mean(tf.square(self.advantage))\n",
        "\n",
        "      # train the value network by minimizing the loss using Adam optimizer\n",
        "      self.train_value_nw = tf.train.AdamOptimizer(0.002).minimize(self.value_loss)\n",
        "\n",
        "    # now, we obtain the policy and its parameter from the policy network\n",
        "    pi, pi_params = self.build_policy_network('pi', trainable=True)\n",
        "\n",
        "    # obtain the old policy and its parameter from the policy network\n",
        "    oldpi, oldpi_params = self.build_policy_network('oldpi', trainable=False)\n",
        "\n",
        "    # sample an action from the new policy\n",
        "    with tf.variable_scope('sample_action'):\n",
        "      self.sample_op = tf.squeeze(pi.sample(1), axis=0)\n",
        "\n",
        "    # update the parameter of old policy\n",
        "    with tf.variable_scope('update_oldpi'):\n",
        "      self.update_oldpi_op = [oldp.assign(p) for p, oldp in zip(pi_params, oldpi_params)]\n",
        "\n",
        "\n",
        "    # define the placeholder for the action\n",
        "    self.action_ph = tf.placeholder(tf.float32, [None, action_shape], 'action')\n",
        "\n",
        "    # define the placeholder for the advantage\n",
        "    self.advantage_ph = tf.placeholder(tf.float32, [None, 1], 'advantage')\n",
        "\n",
        "    # now, let's define our surrogate objective function of the policy network\n",
        "    with tf.variable_scope('loss'):\n",
        "      with tf.variable_scope('surrogate'):\n",
        "\n",
        "        # first let's define the ratio\n",
        "        ratio = pi.prob(self.action_ph) / oldpi.prob(self.action_ph)\n",
        "\n",
        "        # define the objective by multiplying ratio and the advantage function\n",
        "        objective = ratio * self.advantage_ph\n",
        "\n",
        "        # define the objective function with the clipped and unclipped objective:\n",
        "        L = tf.reduce_mean(tf.minimum(objective,\n",
        "                           tf.clip_by_value(ratio, 1. - epsilon, 1. + epsilon) * self.advantage_ph))\n",
        "\n",
        "        # now we can compute the gradient and maximize the objective function using gradient\n",
        "        # ascent. However, instead doing that, we can convert the above maximization objective\n",
        "        # into the minimization objective by just adding a negative sign. So, we can denote the loss of\n",
        "        # the policy network as:\n",
        "\n",
        "        self.policy_loss = - L\n",
        "\n",
        "    # train the policy network by minizing the loss using Adam optimizer:\n",
        "    with tf.variable_scope('train_policy'):\n",
        "      self.train_policy_nw = tf.train.AdamOptimizer(0.001).minimize(self.policy_loss)\n",
        "\n",
        "    # initialize all the tensorflow variables\n",
        "    self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  #now, let's define the train function\n",
        "  def train(self, state, action, reward):\n",
        "\n",
        "    # update the old policy\n",
        "    self.sess.run(self.update_oldpi_op)\n",
        "\n",
        "    # compute the advantage value\n",
        "    adv = self.sess.run(self.advantage, {self.state_ph: state, self.Q: reward})\n",
        "\n",
        "    # train the policy network\n",
        "    [self.sess.run(self.train_policy_nw, {self.state_ph: state, self.action_ph: action, self.advantage_ph: adv}) for _ in range(10)]\n",
        "\n",
        "    # train the value network\n",
        "    [self.sess.run(self.train_value_nw, {self.state_ph: state, self.Q: reward}) for _ in range(10)]\n",
        "\n",
        "    # we define a function called build_policy_network for building the policy network. Note\n",
        "    # that our action space is continuous here, so our policy network returns the mean and\n",
        "    # variance of the action as an output and then we generate a normal distribution using this\n",
        "    # mean and variance and we select an action by sampling from this normal distribution\n",
        "\n",
        "  def build_policy_network(self, name, trainable):\n",
        "    with tf.variable_scope(name):\n",
        "\n",
        "      # define the layer of the network\n",
        "      layer1 = tf.layers.dense(self.state_ph, 100, tf.nn.relu, trainable=trainable)\n",
        "\n",
        "      # compute mean\n",
        "      mu = 2 * tf.layers.dense(layer1, action_shape, tf.nn.tanh, trainable=trainable)\n",
        "\n",
        "      # compute standard deviation\n",
        "      sigma = tf.layers.dense(layer1, action_shape, tf.nn.softplus, trainable=trainable)\n",
        "\n",
        "      # compute the normal distribution\n",
        "      norm_dist = tf.distributions.Normal(loc=mu, scale=sigma)\n",
        "\n",
        "    # get the parameters of the policy network\n",
        "    params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)\n",
        "    return norm_dist, params\n",
        "\n",
        "  # let's define a function called select_action for selecting the action\n",
        "  def select_action(self, state):\n",
        "\n",
        "    state = state[np.newaxis, :]\n",
        "\n",
        "    # sample an action from the normal distribution generated by the policy network\n",
        "    action = self.sess.run(self.sample_op, {self.state_ph: state})[0]\n",
        "\n",
        "    # we clip the action so that they lie within the action bound and then we return the action\n",
        "    action = np.clip(action, action_bound[0], action_bound[1])\n",
        "\n",
        "    return action\n",
        "\n",
        "  # we define a function called get_state_value to obtain the value of the state computed by the value network\n",
        "  def get_state_value(self, state):\n",
        "\n",
        "    if state.ndim < 2: state = state[np.newaxis, :]\n",
        "    return self.sess.run(self.v, {self.state_ph: state})[0, 0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8U1XEJdllckZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traing the Network\n",
        "Now, let's start training the network. First let's create an object to our PPO class:"
      ],
      "metadata": {
        "id": "yzN1JzQQPIEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ppo = PPO()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTP-twZj3Jks",
        "outputId": "ff4a57a3-bf23-4b49-da0f-ae7d41dd098f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-10-a9dfd822815a>:109: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/distributions/normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the number of episodes"
      ],
      "metadata": {
        "id": "20yC2VmuUKtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 1000"
      ],
      "metadata": {
        "id": "s-bXDMbmPrqy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the number of time steps in each episodes"
      ],
      "metadata": {
        "id": "WlMxZXDiUTFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_timesteps = 200"
      ],
      "metadata": {
        "id": "tsaXiX13UQ8c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defin the discount factor, $\\gamma$:"
      ],
      "metadata": {
        "id": "DDkkPxY7Uf6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.9"
      ],
      "metadata": {
        "id": "s6jfKY9XUeX6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the batch size:"
      ],
      "metadata": {
        "id": "b2L-jPRYU0Pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32"
      ],
      "metadata": {
        "id": "I8OQg1P8Ux45"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's train"
      ],
      "metadata": {
        "id": "md8TaIVlU_Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for each episode\n",
        "for i in range(num_episodes):\n",
        "\n",
        "  # Initialize the state by resetting the environment\n",
        "  state = env.reset()\n",
        "\n",
        "  # initialize the lists for holding the states, actions and rewards obtained in the episode\n",
        "  episode_states, episode_actions, episode_rewards = [], [], []\n",
        "\n",
        "  # initialize the return\n",
        "  Return = 0\n",
        "\n",
        "  # for every step\n",
        "  for t in range(num_timesteps):\n",
        "\n",
        "    # render the environment\n",
        "    env.render()\n",
        "\n",
        "    # select the action\n",
        "    action = ppo.select_action(state)\n",
        "\n",
        "    # perform the selected action\n",
        "    next_state, reward, done, _, _= env.step(action)\n",
        "\n",
        "\n",
        "    # store the state, action and reward in the list\n",
        "    episode_states.append(state)\n",
        "    episode_actions.append(action)\n",
        "    episode_rewards.append((reward + 8)/8)\n",
        "\n",
        "    # update the state to the next state\n",
        "    state = next_state\n",
        "\n",
        "    # update the return\n",
        "    Return += reward\n",
        "\n",
        "    # if we reached the batch size or if we reached the final step of the episode\n",
        "    if (t + 1) % batch_size ==0 or t==num_timesteps-1:\n",
        "\n",
        "      # compute the value of the next state\n",
        "      v_s_ = ppo.get_state_value(next_state)\n",
        "\n",
        "      # compute Q value as sum of reward and discounted value of next state\n",
        "      discounted_r = []\n",
        "      for reward in episode_rewards[::-1]:\n",
        "        v_s_ = reward + gamma * v_s_\n",
        "        discounted_r.append(v_s_)\n",
        "      discounted_r.reverse()\n",
        "\n",
        "      # stack the episode states, actions, and rewards:\n",
        "      es, ea, er = np.vstack(episode_states), np.vstack(episode_actions), np.array(discounted_r)[:, np.newaxis]\n",
        "\n",
        "      # empty the lists\n",
        "      episode_states, episode_actions, episode_rewards = [], [], []\n",
        "\n",
        "      # train the network\n",
        "      ppo.train(es, ea, er)\n",
        "\n",
        "\n",
        "  # print the return for every 10 episodes\n",
        "  if i %10 == 0:\n",
        "    print(\"Episode: {}, Return: {}\".format(i, Return))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIXTCVMjU8fJ",
        "outputId": "b3d31585-600e-4e2d-d5fd-1bfd8ee33259"
      },
      "execution_count": 16,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Return: -1154.1664899740276\n",
            "Episode: 10, Return: -1064.2826058693868\n",
            "Episode: 20, Return: -1159.860632680437\n",
            "Episode: 30, Return: -876.0721760548606\n",
            "Episode: 40, Return: -991.6710923246791\n",
            "Episode: 50, Return: -800.6359590527449\n",
            "Episode: 60, Return: -901.6685445915829\n",
            "Episode: 70, Return: -1272.3129613916317\n",
            "Episode: 80, Return: -1451.4365611177159\n",
            "Episode: 90, Return: -1293.7534583663182\n",
            "Episode: 100, Return: -1271.7515109398448\n",
            "Episode: 110, Return: -1324.8206525075707\n",
            "Episode: 120, Return: -1279.4504809659961\n",
            "Episode: 130, Return: -1353.9529036943893\n",
            "Episode: 140, Return: -1405.0746010026148\n",
            "Episode: 150, Return: -1534.2241135946667\n",
            "Episode: 160, Return: -1406.8414557937456\n",
            "Episode: 170, Return: -1235.9155884785223\n",
            "Episode: 180, Return: -1304.7769256225045\n",
            "Episode: 190, Return: -556.2689531229172\n",
            "Episode: 200, Return: -1513.6932241891543\n",
            "Episode: 210, Return: -1469.8544541704157\n",
            "Episode: 220, Return: -1252.5002640204075\n",
            "Episode: 230, Return: -1358.1752593364658\n",
            "Episode: 240, Return: -1177.8890584842331\n",
            "Episode: 250, Return: -524.4551868919799\n",
            "Episode: 260, Return: -1278.506111460463\n",
            "Episode: 270, Return: -1506.359525159552\n",
            "Episode: 280, Return: -1494.377778859252\n",
            "Episode: 290, Return: -1440.6878364342879\n",
            "Episode: 300, Return: -1497.8881117628225\n",
            "Episode: 310, Return: -1103.6632125172355\n",
            "Episode: 320, Return: -1466.7831402120357\n",
            "Episode: 330, Return: -895.7426618889075\n",
            "Episode: 340, Return: -1489.7822941870409\n",
            "Episode: 350, Return: -767.9974664572501\n",
            "Episode: 360, Return: -1041.9550916564947\n",
            "Episode: 370, Return: -0.7050816969599504\n",
            "Episode: 380, Return: -1048.0142652313098\n",
            "Episode: 390, Return: -893.6692070954225\n",
            "Episode: 400, Return: -0.3409288517941529\n",
            "Episode: 410, Return: -748.883880767095\n",
            "Episode: 420, Return: -540.0231117889126\n",
            "Episode: 430, Return: -4.734282559500237\n",
            "Episode: 440, Return: -1021.8805129515497\n",
            "Episode: 450, Return: -745.6208846468803\n",
            "Episode: 460, Return: -513.676637545377\n",
            "Episode: 470, Return: -128.51912187716596\n",
            "Episode: 480, Return: -132.25655361569352\n",
            "Episode: 490, Return: -630.3450586341976\n",
            "Episode: 500, Return: -130.70656421161834\n",
            "Episode: 510, Return: -371.3120006360978\n",
            "Episode: 520, Return: -730.6537463337617\n",
            "Episode: 530, Return: -6.1636772526511034\n",
            "Episode: 540, Return: -126.71222029112852\n",
            "Episode: 550, Return: -1115.0266730451976\n",
            "Episode: 560, Return: -250.51856558884032\n",
            "Episode: 570, Return: -376.1712467283607\n",
            "Episode: 580, Return: -1.2826494727124895\n",
            "Episode: 590, Return: -245.17902667728572\n",
            "Episode: 600, Return: -253.50756164817736\n",
            "Episode: 610, Return: -1097.3859243495522\n",
            "Episode: 620, Return: -127.38407713421431\n",
            "Episode: 630, Return: -909.181266349078\n",
            "Episode: 640, Return: -123.0454691419742\n",
            "Episode: 650, Return: -1194.6902668468176\n",
            "Episode: 660, Return: -1253.5999259887683\n",
            "Episode: 670, Return: -367.5221413963413\n",
            "Episode: 680, Return: -1092.7134833936364\n",
            "Episode: 690, Return: -122.26152279244809\n",
            "Episode: 700, Return: -886.043929068074\n",
            "Episode: 710, Return: -246.1242181199407\n",
            "Episode: 720, Return: -616.4538180625839\n",
            "Episode: 730, Return: -124.80872072339287\n",
            "Episode: 740, Return: -240.50273313096253\n",
            "Episode: 750, Return: -374.63456332219306\n",
            "Episode: 760, Return: -1391.5948222289294\n",
            "Episode: 770, Return: -1208.6470233645914\n",
            "Episode: 780, Return: -369.8543632988133\n",
            "Episode: 790, Return: -1387.9836762355771\n",
            "Episode: 800, Return: -1548.2365463010765\n",
            "Episode: 810, Return: -1454.5924311549313\n",
            "Episode: 820, Return: -1474.5321594673146\n",
            "Episode: 830, Return: -1224.880484954414\n",
            "Episode: 840, Return: -1074.0451086425974\n",
            "Episode: 850, Return: -874.2999572817016\n",
            "Episode: 860, Return: -1.9077865307667778\n",
            "Episode: 870, Return: -257.8982999265195\n",
            "Episode: 880, Return: -1714.3083681173057\n",
            "Episode: 890, Return: -1223.3342258739708\n",
            "Episode: 900, Return: -1251.9716412107202\n",
            "Episode: 910, Return: -977.0913058896824\n",
            "Episode: 920, Return: -1500.708553676701\n",
            "Episode: 930, Return: -125.29559274741472\n",
            "Episode: 940, Return: -124.76614764029927\n",
            "Episode: 950, Return: -1466.1861162891475\n",
            "Episode: 960, Return: -768.5324847410544\n",
            "Episode: 970, Return: -1198.4538822166812\n",
            "Episode: 980, Return: -1739.768732991624\n",
            "Episode: 990, Return: -1389.7470426229054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KV35Lr3St7nQ"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}