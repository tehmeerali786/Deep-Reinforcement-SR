{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM3-e36imQHV",
        "outputId": "80e9952e-32d2-40e6-82a9-f99c3ab3e7e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.13.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import gym"
      ],
      "metadata": {
        "id": "lPAlO2Z8mdl0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO7LUF1dRuBg",
        "outputId": "6b9f5006-e2bb-4000-a8c2-cc9e523e21ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Pendulum-v1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIcwn4LqR_Rt",
        "outputId": "4ef54122-ca42-40eb-c6bf-6cbf80668ed6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_shape = env.observation_space.shape[0]\n",
        "state_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qmdyik96SMmt",
        "outputId": "d955f4f3-3bf0-426c-bfa6-baae8fcdf19e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action_shape = env.action_space.shape[0]\n",
        "action_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjI_U0W2TGZr",
        "outputId": "792e61f9-5919-4ae6-da22-fa325181a8d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pendulum is continuous environment\n",
        "action_bound = [env.action_space.low, env.action_space.high]"
      ],
      "metadata": {
        "id": "-3SslEo9TVYZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_bound"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFDoINIbUHlg",
        "outputId": "3991dbca-55d4-44ba-fe1e-a9333d63393d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-2.], dtype=float32), array([2.], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9_Cqm3u6UItU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Variables"
      ],
      "metadata": {
        "id": "VvHe5QJfUmL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.9"
      ],
      "metadata": {
        "id": "gmmuZcl5UpSj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tau = 0.01"
      ],
      "metadata": {
        "id": "BdnCinAVUxwj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replay_buffer = 10000"
      ],
      "metadata": {
        "id": "5gbEn2h5U10i"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32"
      ],
      "metadata": {
        "id": "mETA1Q2qVS-g"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the DDPG class"
      ],
      "metadata": {
        "id": "PJzMGjIPVoiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DDPG(object):\n",
        "\n",
        "  #first, let's define the init method\n",
        "  def __init__(self, state_shape, action_shape, high_action_value,):\n",
        "\n",
        "\n",
        "    # define the replay buffer for storing the transitions\n",
        "    self.replay_buffer = np.zeros((replay_buffer, state_shape * 2 + action_shape + 1))\n",
        "\n",
        "    # initialize the num_transitionsto 0 which implies that the number of transitions in our\n",
        "    # replay buffer is zero\n",
        "    self.num_transitions = 0\n",
        "\n",
        "    # start the Tensorflow session\n",
        "    self.sess = tf.Session()\n",
        "\n",
        "    # we learned that in DDPG, instead of selecting the action directly, to ensure expoloration\n",
        "    # we add some noise using the Ornstein-Uhlenbeck process. So, we first initialize the noise\n",
        "    self.noise = 3.0\n",
        "\n",
        "    # initialize the state shape, action shape, and high action value\n",
        "    self.state_shape, self.action_shape, self.high_action_value = state_shape, action_shape, high_action_value\n",
        "\n",
        "    # define the placeholder of the state\n",
        "    self.state = tf.placeholder(tf.float32, [None, state_shape], 'state')\n",
        "\n",
        "    # define the placehodler for the next state\n",
        "    self.next_state = tf.placeholder(tf.float32, [None, state_shape], 'next_state')\n",
        "    # define the placeholder for the reward\n",
        "    self.reward = tf.placeholder(tf.float32, [None, 1], 'reward')\n",
        "\n",
        "    # with the actor variable scope\n",
        "    with tf.variable_scope('Actor'):\n",
        "\n",
        "      # define the main actor network which is parameterized by phi. Actor network takes the state\n",
        "      # as an input and returns the action to be performed in that state\n",
        "      self.actor = self.build_actor_network(self.state, scope='main', trainable=True)\n",
        "\n",
        "      # Define the target actor network which is parameterized by phi dash. Target actor network takes\n",
        "      # the next state as an input and returns the action to be performed in that state\n",
        "      target_actor = self.build_actor_network(self.next_state, scope='target', trainable=False)\n",
        "\n",
        "\n",
        "    # with the critic variable scope\n",
        "    with tf.variable_scope('Critic'):\n",
        "\n",
        "      # define the main critic network which is parameterized by theta. Critic network takes the state\n",
        "      # and also the action produced by the actor in that state as an input and returns the Q-Value\n",
        "      critic = self.build_critic_network(self.state, self.actor, scope='main', trainable=True)\n",
        "\n",
        "      # Define the target critic network which is parameterized by theta dash. Target critic network takes\n",
        "      # the next state and also the action produced by the target actor network in the next state as\n",
        "      # an input and returns the Q value\n",
        "      target_critic = self.build_critic_network(self.next_state, target_actor, scope='target', trainable=False)\n",
        "\n",
        "    # get the parameter of the main actor network, phi\n",
        "    self.main_actor_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/main')\n",
        "\n",
        "    # get the parameter of the target actor network, phi dash\n",
        "    self.target_actor_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n",
        "\n",
        "    # get the parameter of the main critic network, theta\n",
        "    self.main_critic_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/main')\n",
        "\n",
        "    # get the parameter of the target critic network, theta dash\n",
        "    self.target_critic_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n",
        "\n",
        "    # perform the soft replacement and update the parameter of the target actor network and\n",
        "    # the parameter of the target critic network\n",
        "    self.soft_replacement = [\n",
        "\n",
        "\n",
        "              [tf.assign(phi_, tau*phi + (1-tau) * phi_), tf.assign(theta_, tau*theta + (1-tau) * theta_)]\n",
        "              for phi, phi_, theta, theta_ in zip(self.main_actor_params, self.target_actor_params, self.main_critic_params, self.target_critic_params)\n",
        "    ]\n",
        "\n",
        "    # compute the target Q value, we learned that the target Q value can be computed as the\n",
        "    # sum of reward and discounted Q value of next state-action pair\n",
        "    y = self.reward + gamma * target_critic\n",
        "\n",
        "    # now, let's compute the loss of the critic network. The loss of the critic network is the mean\n",
        "    # squared error between the target Q value and the predicted Q value\n",
        "    MSE = tf.losses.mean_squared_error(labels=y, predictions=critic)\n",
        "\n",
        "    # train the critic network by minimizing the mean squared error using Adam optimizer\n",
        "    self.train_critic = tf.train.AdamOptimizer(0.01).minimize(MSE, name='adam-ink', var_list = self.main_critic_params)\n",
        "\n",
        "    # We learned that the objective function of the actor is to generate an action that maximizes\n",
        "    # the Q value produced by the critic network. We can maximize the above objective by computing gradients\n",
        "    # and performing the gradient ascent. However, it is a standard convention to perform minimization rather\n",
        "    # than maximization. So, we can convert the above maximiation objective into the minimization\n",
        "    # objective by just adding a negative sign\n",
        "\n",
        "    # now we can minimize the actor network objective by computing the gradients and by performing gradient descent\n",
        "    actor_loss = -tf.reduce_mean(critic)\n",
        "\n",
        "    # train the actor network by minimizing the loss using Adam Optimizer\n",
        "    self.train_actor = tf.train.AdamOptimizer(0.001).minimize(actor_loss, var_list=self.main_actor_params)\n",
        "\n",
        "    # initialize all the Tensorflow variables\n",
        "    self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "  # let's define a function called select_action for selecting the action with the noise to ensure exploration\n",
        "  def select_action(self, state):\n",
        "\n",
        "    # run the actor network and get the action\n",
        "    action = self.sess.run(self.actor, {self.state: state[np.newaxis, :]})[0]\n",
        "\n",
        "\n",
        "    # now we generate a normal distribution with mean as action and standard deviation as the\n",
        "    # noise. Then, we randomly select an action from this normal distribution\n",
        "    action = np.random.normal(action, self.noise)\n",
        "\n",
        "    # we need to make sure that our action should not fall away from the action bound. So, we\n",
        "    # clip the action so that they lie within the action bound and then we return the action\n",
        "    action = np.clip(action, action_bound[0], action_bound[1])\n",
        "\n",
        "    return action\n",
        "\n",
        "  # now, let's define the train function\n",
        "  def train(self):\n",
        "\n",
        "    # perform the soft replacement\n",
        "    self.sess.run(self.soft_replacement)\n",
        "\n",
        "    # randomly select the indices from the replay buffer with the given batch size\n",
        "    indices = np.random.choice(replay_buffer, size=batch_size)\n",
        "\n",
        "    # select the batch of transitions from the replay buffer with the selected indices\n",
        "    batch_transition = self.replay_buffer[indices, :]\n",
        "\n",
        "    # get the batch of states, actions, rewards, and next states\n",
        "    batch_states = batch_transition[:, :self.state_shape]\n",
        "    batch_actions = batch_transition[:, self.state_shape: self.state_shape + self.action_shape]\n",
        "    batch_rewards = batch_transition[:, -self.state_shape - 1: -self.state_shape]\n",
        "    batch_next_state = batch_transition[:, -self.state_shape:]\n",
        "\n",
        "    # train the actor network\n",
        "    self.sess.run(self.train_actor, {self.state: batch_states})\n",
        "\n",
        "    # train the actor network\n",
        "    self.sess.run(self.train_critic, {self.state: batch_states, self.actor: batch_actions,\n",
        "                                      self.reward: batch_rewards, self.next_state: batch_next_state})\n",
        "\n",
        "\n",
        "  # now, let's store the transitions in the replay buffer\n",
        "  def store_transition(self, state, actor, reward, next_state):\n",
        "\n",
        "    # first stack the state, action, reward, and next state\n",
        "    trans = np.hstack((state, actor, [reward], next_state))\n",
        "\n",
        "    # get the index\n",
        "    index = self.num_transitions % replay_buffer\n",
        "\n",
        "    # store the transition\n",
        "    self.replay_buffer[index, :] = trans\n",
        "\n",
        "    # update the number of transitions\n",
        "    self.num_transitions = self.num_transitions + 1\n",
        "\n",
        "    # if the number of transitions is greater than the replay buffer then train the network\n",
        "    if self.num_transitions > replay_buffer:\n",
        "      self.noise = self.noise * 0.99995\n",
        "      self.train()\n",
        "\n",
        "\n",
        "  def build_actor_network(self, state, scope, trainable):\n",
        "\n",
        "    # we define a function called build_actor_network for building the actor network. The\n",
        "    # actor network takes the state and returns the action to be performed in that state\n",
        "\n",
        "    with tf.variable_scope(scope):\n",
        "      layer_1 = tf.layers.dense(state, 30, activation=tf.nn.tanh, name='layer_1', trainable=trainable)\n",
        "      actor = tf.layers.dense(layer_1, self.action_shape, activation=tf.nn.tanh, name='actor', trainable=trainable)\n",
        "      return tf.multiply(actor, self.high_action_value, name='scaled_a')\n",
        "\n",
        "\n",
        "  def build_critic_network(self, state, actor, scope, trainable):\n",
        "\n",
        "    # we define a function called build_critic_nework for building the critic network.\n",
        "    # The critic network takes the state and the action produced by the actor in that state and returns the Q value\n",
        "    with tf.variable_scope(scope):\n",
        "      w1_s = tf.get_variable('w1_s', [self.state_shape, 30] , trainable=trainable)\n",
        "      w1_a = tf.get_variable('w1_a', [self.action_shape, 30], trainable=trainable)\n",
        "      b1 = tf.get_variable('b1', [1, 30], trainable=trainable)\n",
        "      net = tf.nn.tanh(tf.matmul(state, w1_s) + tf.matmul(actor, w1_a) + b1)\n",
        "\n",
        "      critic = tf.layers.dense(net, 1, trainable=trainable)\n",
        "      return critic\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z7VzSriqVVXw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h3GrKebNEtR4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Network"
      ],
      "metadata": {
        "id": "quyrvbr6FqvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an object\n",
        "ddpg = DDPG(state_shape, action_shape, action_bound[1])"
      ],
      "metadata": {
        "id": "bHEjA0JgFtjT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the number of episodes\n",
        "num_episodes = 300"
      ],
      "metadata": {
        "id": "nU6QKoV-F4Nz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the number of timesteps\n",
        "num_timesteps = 500"
      ],
      "metadata": {
        "id": "097qA-JJJOTX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for each episode\n",
        "for i in range(num_episodes):\n",
        "\n",
        "  # initialize the state by resetting the environment\n",
        "  state = env.reset()\n",
        "\n",
        "  # initialize the return\n",
        "  Return = 0\n",
        "\n",
        "  # for every step\n",
        "  for j in range(num_timesteps):\n",
        "\n",
        "    # render the environment\n",
        "    env.render()\n",
        "\n",
        "    # select the action\n",
        "    action = ddpg.select_action(state)\n",
        "\n",
        "    # perform the selected action\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "\n",
        "    # store the transition in the replay buffer\n",
        "    ddpg.store_transition(state, action, reward, next_state)\n",
        "\n",
        "    # update the return\n",
        "    Return = Return + reward\n",
        "\n",
        "    # if the state is the terminal state then break\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "    # update the state to the next state\n",
        "    state = next_state\n",
        "\n",
        "\n",
        "  # print the return for every 10 episodes\n",
        "  if i % 10 ==0:\n",
        "    print('Episodes: {}, Return: {}'.format(i, Return))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDiG90APJTEm",
        "outputId": "e57689b6-5e5d-491e-d981-b11429b84721"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episodes: 0, Return: -1298.5162089197013\n",
            "Episodes: 10, Return: -1344.0541317113523\n",
            "Episodes: 20, Return: -1024.741697866924\n",
            "Episodes: 30, Return: -1194.321818697804\n",
            "Episodes: 40, Return: -1546.1128284571869\n",
            "Episodes: 50, Return: -1758.1434513936158\n",
            "Episodes: 60, Return: -1165.3305322425601\n",
            "Episodes: 70, Return: -629.5084814605667\n",
            "Episodes: 80, Return: -504.074356965642\n",
            "Episodes: 90, Return: -503.94235890850075\n",
            "Episodes: 100, Return: -255.03846410424228\n",
            "Episodes: 110, Return: -376.51510289232505\n",
            "Episodes: 120, Return: -590.9656987977198\n",
            "Episodes: 130, Return: -592.8878173784373\n",
            "Episodes: 140, Return: -125.29076048949426\n",
            "Episodes: 150, Return: -125.60270949027333\n",
            "Episodes: 160, Return: -1.6196354001105746\n",
            "Episodes: 170, Return: -129.4081757558872\n",
            "Episodes: 180, Return: -130.42261095419096\n",
            "Episodes: 190, Return: -1439.4973994807433\n",
            "Episodes: 200, Return: -126.80733034595998\n",
            "Episodes: 210, Return: -130.4687847364974\n",
            "Episodes: 220, Return: -128.5719074640325\n",
            "Episodes: 230, Return: -272.4646088863576\n",
            "Episodes: 240, Return: -132.1589779858584\n",
            "Episodes: 250, Return: -128.59245721362925\n",
            "Episodes: 260, Return: -134.42745758010193\n",
            "Episodes: 270, Return: -373.1401193306782\n",
            "Episodes: 280, Return: -280.0909524976399\n",
            "Episodes: 290, Return: -136.36455740399958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Pendulum-v1\").unwrapped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwBNiYPBMGXr",
        "outputId": "51a039c6-e3ae-4660-e2c0-fd264183cc2f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = env.unwrapped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tu9vJTWNIB4",
        "outputId": "8e6027f2-667e-43d4-9499-9a405a26fb41"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_72h3zRNPA3",
        "outputId": "d69b019c-2f17-4d41-b7c9-2c4ab3cf1786"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.47130384])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "# Create the Pendulum environment\n",
        "env = gym.make('Pendulum-v1').unwrapped\n",
        "\n",
        "# Set the environment to the initial state\n",
        "state = env.reset()\n",
        "\n",
        "# Run the simulation for 10 time steps\n",
        "for t in range(10):\n",
        "    # Render the environment\n",
        "    env.render()\n",
        "\n",
        "    # Take a random action\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, done, info = env.step(action)\n",
        "\n",
        "# Close the environment\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "oYgXkWcINP_n",
        "outputId": "e9a2cdaa-2bc3-4b81-817c-18718bb71d72"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-083a85ccd0fd>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Take a random action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Close the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q89GTfHWNX0n",
        "outputId": "2b2e1fe9-dfae-410d-f255-f7a192e03799"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.5769268, -0.8167958, -2.9895468], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3L8EwEAO1Jm",
        "outputId": "4b848aee-29fa-4498-9f3c-19ea9aaca6a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1.2961853646320602"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiIyWvpuO5qx",
        "outputId": "bd7fa72d-6024-40e7-cf60-8eb67b2ae06f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iQ661qDO8-R",
        "outputId": "8cd862f6-fd72-4f55-c586-3711e88574bc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XDTHTFliO92R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}