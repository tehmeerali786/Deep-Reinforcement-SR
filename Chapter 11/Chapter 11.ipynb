{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "slOo7hW0qAZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8917f0e-13df-4617-9d9b-88447d8c34d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import gym\n",
        "import multiprocessing\n",
        "import threading\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ADwsKu76q3K5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tVM6cpTrDja",
        "outputId": "516c38ef-d304-4427-ddd9-e560dbba88a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Car Environment"
      ],
      "metadata": {
        "id": "mcqjzfvPvlrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a mountain car environment using the gym. Note that our mountain car environment is a coutinous environment. It means, our action space is countious"
      ],
      "metadata": {
        "id": "HgzwwE5Gvl0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('MountainCarContinuous-v0')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bm7CH4pvicx",
        "outputId": "6c0eb362-f58e-4050-faa4-310cd22ee9cb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_shape = env.observation_space.shape[0]\n",
        "state_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkMnWHcEvili",
        "outputId": "5ddfd2c9-47a3-4dea-a411-1fe47fc3c61d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action_shape = env.action_space.shape[0]"
      ],
      "metadata": {
        "id": "4G68iO6xu73K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnFF3abjx7Wt",
        "outputId": "2c98257a-68ff-48a3-bc50-84c3dcd98f9d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we created the continuous mountain car environment and thus our action space consists of continuous values. So, we get the bound of our action\n",
        "space."
      ],
      "metadata": {
        "id": "3IHMPg-0yHJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action_bound = [env.action_space.low, env.action_space.high]\n",
        "action_bound"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JdEeCyex_WM",
        "outputId": "dc05b198-6cb5-47b9-8c1f-717d1b5747e0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-1.], dtype=float32), array([1.], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Variables"
      ],
      "metadata": {
        "id": "9uCUCUdbzsqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of workers as the number of CPUs\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "num_workers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peJ7GxBQytAa",
        "outputId": "57d17249-750e-45c1-d820-4e7e2c8865a8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 2000"
      ],
      "metadata": {
        "id": "sYfTep7y0Em1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_timesteps = 200"
      ],
      "metadata": {
        "id": "qZoMYazi0OZT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Global Network Scope\n",
        "global_net_scope = 'Global_Net'"
      ],
      "metadata": {
        "id": "Khassxn60TaT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the time step at which we want to update the global network\n",
        "update_global = 10"
      ],
      "metadata": {
        "id": "5OEBxDFk1X4v"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.90"
      ],
      "metadata": {
        "id": "zUyK3Nm71wGu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beta = 0.01"
      ],
      "metadata": {
        "id": "Gol2gA7K12Zd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = 'logs'"
      ],
      "metadata": {
        "id": "Ef0PUZAX2SSN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the actor critic class"
      ],
      "metadata": {
        "id": "4E2WQARx2vXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We learned that A3C both the global and worker agents follow the actor critic architecture. So, let's define the class ActorCritic. Here, we will implement the actor critic algorithm.\n",
        "\n"
      ],
      "metadata": {
        "id": "Sb5aeKRN21Zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic(object):\n",
        "\n",
        "     #first, let's define the init method\n",
        "     def __init__(self, scope, sess, globalAC=None):\n",
        "\n",
        "        #initialize the TensorFlow session\n",
        "        self.sess=sess\n",
        "\n",
        "        #define the actor network optimizer as RMS prop\n",
        "        self.actor_optimizer = tf.train.RMSPropOptimizer(0.0001, name='RMSPropA')\n",
        "\n",
        "        #define the critic network optimizer as RMS prop\n",
        "        self.critic_optimizer = tf.train.RMSPropOptimizer(0.001, name='RMSPropC')\n",
        "\n",
        "        #if the scope is the global network (global agent)\n",
        "        if scope == global_net_scope:\n",
        "            with tf.variable_scope(scope):\n",
        "\n",
        "                #define the placeholder for the state\n",
        "                self.state = tf.placeholder(tf.float32, [None, state_shape], 'state')\n",
        "\n",
        "                #build the global network (global agent) and get the actor and critic parameters\n",
        "                self.actor_params, self.critic_params = self.build_network(scope)[-2:]\n",
        "\n",
        "        #if the network is not the global network then\n",
        "        else:\n",
        "            with tf.variable_scope(scope):\n",
        "\n",
        "                #define the placeholder for the state\n",
        "                self.state = tf.placeholder(tf.float32, [None, state_shape], 'state')\n",
        "\n",
        "                #we learned that our environment is the continuous environment, so the actor network\n",
        "                #(policy network) returns the mean and variance of the action and then we build the action\n",
        "                #distribution out of this mean and variance and select the action based on this action\n",
        "                #distribution.\n",
        "\n",
        "                #define the placeholder for obtaining the action distribution\n",
        "                self.action_dist = tf.placeholder(tf.float32, [None, action_shape], 'action')\n",
        "\n",
        "                #define the placeholder for the target value\n",
        "                self.target_value = tf.placeholder(tf.float32, [None, 1], 'Vtarget')\n",
        "\n",
        "                #build the worker network (worker agent) and get the mean and variance of the action, the\n",
        "                #value of the state, and actor and critic network parameters:\n",
        "                mean, variance, self.value, self.actor_params, self.critic_params = self.build_network(scope)\n",
        "\n",
        "                #Compute the TD error which is the difference between the target value of the state and the\n",
        "                #predicted value of the state\n",
        "                td_error = tf.subtract(self.target_value, self.value, name='TD_error')\n",
        "\n",
        "                #now, let's define the critic network loss\n",
        "                with tf.name_scope('critic_loss'):\n",
        "                    self.critic_loss = tf.reduce_mean(tf.square(td_error))\n",
        "\n",
        "                with tf.name_scope('wrap_action'):\n",
        "                    mean, variance = mean * action_bound[1], variance + 1e-4\n",
        "\n",
        "                #create a normal distribution based on the mean and variance of the action\n",
        "                normal_dist = tf.distributions.Normal(mean, variance)\n",
        "\n",
        "\n",
        "                #now, let's define the actor network loss\n",
        "                with tf.name_scope('actor_loss'):\n",
        "\n",
        "                    #compute the log probability of the action\n",
        "                    log_prob = normal_dist.log_prob(self.action_dist)\n",
        "\n",
        "                    #define the entropy of the policy\n",
        "                    entropy_pi = normal_dist.entropy()\n",
        "\n",
        "                    #compute the actor network loss\n",
        "                    self.loss = log_prob * td_error + (beta * entropy_pi)\n",
        "                    self.actor_loss = tf.reduce_mean(-self.loss)\n",
        "\n",
        "                #select the action based on the normal distribution\n",
        "                with tf.name_scope('select_action'):\n",
        "                    self.action = tf.clip_by_value(tf.squeeze(normal_dist.sample(1), axis=0),\n",
        "                                                   action_bound[0], action_bound[1])\n",
        "\n",
        "\n",
        "                #compute the gradients of actor and critic network loss of the worker agent (local agent)\n",
        "                with tf.name_scope('local_grad'):\n",
        "\n",
        "                    self.actor_grads = tf.gradients(self.actor_loss, self.actor_params)\n",
        "                    self.critic_grads = tf.gradients(self.critic_loss, self.critic_params)\n",
        "\n",
        "            #now, let's perform the sync operation\n",
        "            with tf.name_scope('sync'):\n",
        "\n",
        "                #after computing the gradients of the loss of the actor and critic network, worker agent\n",
        "                #sends (push) those gradients to the global agent\n",
        "                with tf.name_scope('push'):\n",
        "                    self.update_actor_params = self.actor_optimizer.apply_gradients(zip(self.actor_grads,\n",
        "                                                                                        globalAC.actor_params))\n",
        "                    self.update_critic_params = self.critic_optimizer.apply_gradients(zip(self.critic_grads,\n",
        "                                                                                          globalAC.critic_params))\n",
        "\n",
        "                #global agent updates their parameter with the gradients received from the worker agents\n",
        "                #(local agents). Then the worker agents, pull the updated parameter from the global agent\n",
        "                with tf.name_scope('pull'):\n",
        "                    self.pull_actor_params = [l_p.assign(g_p) for l_p, g_p in zip(self.actor_params,\n",
        "                                                                                  globalAC.actor_params)]\n",
        "                    self.pull_critic_params = [l_p.assign(g_p) for l_p, g_p in zip(self.critic_params,\n",
        "                                                                                   globalAC.critic_params)]\n",
        "\n",
        "\n",
        "     #let's define the function for building the actor critic network\n",
        "     def build_network(self, scope):\n",
        "\n",
        "        #initialize the weight:\n",
        "        w_init = tf.random_normal_initializer(0., .1)\n",
        "\n",
        "        #define the actor network which returns the mean and variance of the action\n",
        "        with tf.variable_scope('actor'):\n",
        "            l_a = tf.layers.dense(self.state, 200, tf.nn.relu, kernel_initializer=w_init, name='la')\n",
        "            mean = tf.layers.dense(l_a, action_shape, tf.nn.tanh,kernel_initializer=w_init, name='mean')\n",
        "            variance = tf.layers.dense(l_a, action_shape, tf.nn.softplus, kernel_initializer=w_init, name='variance')\n",
        "\n",
        "        #define the critic network which returns the value of the state\n",
        "        with tf.variable_scope('critic'):\n",
        "            l_c = tf.layers.dense(self.state, 100, tf.nn.relu, kernel_initializer=w_init, name='lc')\n",
        "            value = tf.layers.dense(l_c, 1, kernel_initializer=w_init, name='value')\n",
        "\n",
        "        actor_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')\n",
        "        critic_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')\n",
        "\n",
        "        #Return the mean and variance of the action produced by the actor network, value of the\n",
        "        #state computed by the critic network and the parameters of the actor and critic network\n",
        "\n",
        "        return mean, variance, value, actor_params, critic_params\n",
        "\n",
        "     #let's define a function called update_global for updating the parameters of the global\n",
        "     #network with the gradients of loss computed by the worker networks, that is, the push operation\n",
        "     def update_global(self, feed_dict):\n",
        "        self.sess.run([self.update_actor_params, self.update_critic_params], feed_dict)\n",
        "\n",
        "     #we also define a function called pull_from_global for updating the parameters of the\n",
        "     #worker networks by pulling from the global network, that is, the pull operation\n",
        "     def pull_from_global(self):\n",
        "        self.sess.run([self.pull_actor_params, self.pull_critic_params])\n",
        "\n",
        "     #define a function called select_action for selecting the action\n",
        "     def select_action(self, state):\n",
        "        state = state[np.newaxis, :]\n",
        "        return self.sess.run(self.action, {self.state: state})[0]"
      ],
      "metadata": {
        "id": "sIFU63zN2Ym7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Worker Class"
      ],
      "metadata": {
        "id": "mvplzqhmVykT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define the class called Worker when we will implement the worker gaint:"
      ],
      "metadata": {
        "id": "OESghdh3V3lU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Worker(object):\n",
        "\n",
        "  # first let's define the init method\n",
        "  def __init__(self, name, globalAC, sess):\n",
        "\n",
        "    # we learned that each worker agent works with their own copies of the environment. So,\n",
        "    # let's create a mountain car environment\n",
        "    self.env = gym.make('MountainCarContinuous-v0').unwrapped\n",
        "\n",
        "    # define the name of the worker\n",
        "    self.name = name\n",
        "\n",
        "    # create an object to our ActorCritic class\n",
        "    self.AC = ActorCritic(name, sess, globalAC)\n",
        "\n",
        "    # initialize a Tensorflow session\n",
        "    self.sess = sess\n",
        "\n",
        "  # define a function called work for the worker to learn\n",
        "  def work(self):\n",
        "    global global_rewards, global_episodes\n",
        "\n",
        "    # initialize the time step\n",
        "    total_step = 1\n",
        "\n",
        "    # initialize a list for storing the states, actions and rewards\n",
        "    batch_states, batch_actions, batch_rewards = [], [], []\n",
        "\n",
        "    # when the global episodes are less than the number of episodes and coordinator is active\n",
        "    while not coord.should_stop and global_episodes < num_episodes:\n",
        "\n",
        "      # initialize the state by resetting the environment\n",
        "      state = self.env.reset()\n",
        "\n",
        "      # initialize the return\n",
        "      Return = 0\n",
        "\n",
        "      # for each step in the environment\n",
        "      for t in range(num_timesteps):\n",
        "\n",
        "        # render the environment of only the worker 0:\n",
        "        if self.name == 'W_0':\n",
        "          self.env.render()\n",
        "\n",
        "        # select the action\n",
        "        action = self.AC.select_action(state)\n",
        "\n",
        "        # perform the selected action\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "        # set done to true if we reached the final step of the episode else set to false\n",
        "        done = True if t==num_timesteps -1 else False\n",
        "\n",
        "        # update the return\n",
        "        Return = Return + reward\n",
        "\n",
        "        # store the state, action, and reward into the lists\n",
        "        batch_states.append(state)\n",
        "        batch_actions.append(actions)\n",
        "        batch_rewards.append((reward+8)/8)\n",
        "\n",
        "        # now let's updat the global network. If done is true then set the value of the next state to 0 else\n",
        "        # the compute values of the next state\n",
        "        if total_step % update_global == 0 or done:\n",
        "          if done:\n",
        "            v_s_ = 0\n",
        "          else:\n",
        "            v_s_ = self.sess.run(self.AC.value, {self.AC.state: next_state[np.newaxis, :]})[0, 0]\n",
        "\n",
        "          batch_target_value = []\n",
        "\n",
        "          # compute the target value which is sum of reward and discounted value of next state\n",
        "          for reward in batch_rewards[::-1]:\n",
        "            v_s_ = reward + gamma * v_s_\n",
        "            batch_target_value.append(v_s_)\n",
        "\n",
        "          # reverse the target value\n",
        "          batch_target_value.reverse()\n",
        "\n",
        "          # stack the state, action and target value\n",
        "          batch_states, batch_actions, batch_target_value = np.vstack(batch_states), np.vstack(batch_actions), np.vstack(batch_target_value)\n",
        "\n",
        "          # define the feed dictionary\n",
        "          feed_dict = {\n",
        "\n",
        "                       self.AC.state: batch_states,\n",
        "                       self.AC.action_dist: batch_actions,\n",
        "                       self.AC.target_value: batch_target_value,\n",
        "          }\n",
        "\n",
        "          # update the global network\n",
        "          self.AC.update_global(feed_dict)\n",
        "\n",
        "          # empty the lists:\n",
        "          batch_states, batch_actions, batch_rewards = [], [], []\n",
        "\n",
        "          # update the worker network by pulling the parameters from the global network:\n",
        "          self.AC.pull_from_global()\n",
        "\n",
        "      # update the state to the next state and increment the total steps:\n",
        "      state = next_state\n",
        "      total_step = total_step + 1\n",
        "\n",
        "      # update global rewards\n",
        "      if done:\n",
        "        if len(global_rewards) < 5:\n",
        "          global_rewards.append(Return)\n",
        "\n",
        "        else:\n",
        "          global_rewards.append(Return)\n",
        "          global_rewards[-1] = (np.mean(global_rewards[-5:]))\n",
        "\n",
        "        global_episodes = global_episodes + 1\n",
        "        break\n",
        "\n"
      ],
      "metadata": {
        "id": "ZJr8YvMDPT6c"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Network"
      ],
      "metadata": {
        "id": "xYBIzmQkFLDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hIg9UUHLFQLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_rewards = []\n",
        "global_episodes = 0"
      ],
      "metadata": {
        "id": "ihHJu5MSCQzA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sess = tf.Session()"
      ],
      "metadata": {
        "id": "QReBhcm7FXuT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device(\"/cpu:0\"):\n",
        "\n",
        "  # create a global agent\n",
        "  global_agent = ActorCritic(global_net_scope, sess)\n",
        "  worker_agents = []\n",
        "\n",
        "  # create n number of worker agent:\n",
        "  for i in range(num_workers):\n",
        "    i_name = 'W_%i' % i\n",
        "    worker_agents.append(Worker(i_name, global_agent, sess))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1ZLgO1KFj3C",
        "outputId": "8fca2410-a869-49fb-cb57-1d50d8d7cac9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-17-836e004bd1a0>:59: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/distributions/normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Tensorflow Coordinator\n",
        "coord = tf.train.Coordinator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcKpFUSvGnHe",
        "outputId": "68f6376b-6975-47b7-e22e-e0bf5b5b3e14"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the tensorflow variables\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4loh8AvJxNj",
        "outputId": "9da04bbf-6154-49b9-a6ec-26cfbc4a6a52"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the Tensorflow comptational graph to the log directory\n",
        "if os.path.exists(log_dir):\n",
        "  shutil.rmtree(log_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxeJyBtLJ8Uh",
        "outputId": "cd66ee3a-34e3-4d31-883f-0a478dcb9596"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.summary.FileWriter(log_dir, sess.graph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXGF61sdKclw",
        "outputId": "c200cdbb-c0f6-4dad-8319-e7695fda77a3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.summary.writer.writer.FileWriter at 0x78e4e6ed5660>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now run the worker threads\n",
        "worker_threads = []\n",
        "for worker in worker_agents:\n",
        "\n",
        "  job = lambda: worker.work()\n",
        "  thread = threading.Thread(target=job)\n",
        "  thread.start()\n",
        "  worker_threads.append(thread)\n",
        "\n",
        "\n",
        "coord.join(worker_threads)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhV0GOAwKppe",
        "outputId": "ef519684-ba26-40ff-80f9-7973a4e5d668"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Urhvf_tjLYtM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}